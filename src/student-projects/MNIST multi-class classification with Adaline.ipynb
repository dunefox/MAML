{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification: Linear classification with the MNIST data set\n",
    "\n",
    "Author: Severin Angerpointner\n",
    "\n",
    "**Abstract:** In this tutorial we will:\n",
    "* Implement a layer of Adalines (neurons) for multiclass classification (here with the MNIST dataset of handwritten digits)\n",
    "* Carry out the training\n",
    "* Set an efficiency benchmark in number recognition for comparison to a multi-layer network (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 8]\n",
    "matplotlib.rc(\"savefig\", dpi=200)\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "disable_js = \"\"\"\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\"\"\"\n",
    "display(Javascript(disable_js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module for file access\n",
    "import os.path\n",
    "\n",
    "# global file name of our data source\n",
    "digit_name = 'digits.csv'\n",
    "label_name = 'labels.csv'\n",
    "\n",
    "def fetch_digit_data(num_images):\n",
    "    '''\n",
    "    Download 'num_images' datasets (images plus labels) from the MNIST database and save them as .csv files for further use.\n",
    "    (This might take a while for the whole dataset (60,000 images))\n",
    "    '''\n",
    "    #open gzip file from url and unzip it\n",
    "    with gzip.open(\n",
    "                  urllib.request.urlopen(\n",
    "                      'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')   ) as bytestream:\n",
    "        #read raw bytestring in buffer (images are 28 by 28 pixels)\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8)\n",
    "        data = data.reshape((num_images, 28 * 28))\n",
    "        #save images as num_images x 28*28 array\n",
    "        np.savetxt(digit_name, data, fmt='%u',delimiter=',')\n",
    "  \n",
    "    #analogous for the labels\n",
    "    with gzip.open(\n",
    "                  urllib.request.urlopen(\n",
    "                      'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')   ) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8)\n",
    "        data = data.reshape(num_images)\n",
    "        np.savetxt(label_name, data, fmt='%u',delimiter=',')\n",
    "         \n",
    "    return\n",
    "\n",
    "# fetch data from internet source only if the file is not available locally\n",
    "#fetch both and override old data to ensure same ordering\n",
    "if not os.path.exists(digit_name) or not os.path.exists(label_name):\n",
    "    fetch_digit_data(60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the multiclass data\n",
    "\n",
    "The `prep_digit_data()` function has to be adapted as in the preceding implementations we only used two classes (eg. for iris data). The class labels will now be [0,9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_digit_data(df,dl):\n",
    "    ''' \n",
    "    Fetches 28x28 digit images from the digit data and\n",
    "    returns (X, Y), where X is a list of 28x28 arrays and Y a list of labels.\n",
    "    '''\n",
    "   \n",
    "    # as feature we take the first 28 * 28 data entries,\n",
    "    # which are the pixel brightness/darkness values (between 0 and 255)\n",
    "    X = df.iloc[:, 0:784].values\n",
    "    \n",
    "\n",
    "    # read class labels: (labels already suitable indices, between 0 and 9)\n",
    "    classes = dl.iloc[:].values \n",
    "\n",
    "    Y = []\n",
    "    for i in range(len(classes)):\n",
    "        Y.append(classes[i])\n",
    "    \n",
    "    # to make it more realistic, we randomize the data\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X_rand = [X[i] for i in indices]\n",
    "    Y_rand = [Y[i] for i in indices]\n",
    "\n",
    "    # return the randomized lists as numpy arrays\n",
    "    return np.array(X_rand,dtype=np.float64), np.array(Y_rand)\n",
    "\n",
    "# This function is not in use (not revised yet)\n",
    "def generate_random_data(num, n_features=2, n_classes=2):\n",
    "    '''\n",
    "    generates num items of radom data\n",
    "    Y: an array of classes '-1 or 1'\n",
    "    X: the corresponding data vectors\n",
    "    '''\n",
    "    \n",
    "    # generate class labels -1 and 1\n",
    "    Y = np.random.randint(n_classes, size=num)\n",
    "\n",
    "    # generate the data on basis of which the distinction should be made\n",
    "    X = np.random.rand(num, n_features)\n",
    "\n",
    "    for n in range(num):\n",
    "        phi = 2 * np.pi / (n_classes) * (Y[n] + (np.random.rand()-.5))\n",
    "        v = np.array([np.cos(phi), np.sin(phi)])\n",
    "        X[n, :] = v * np.random.rand() * 2\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Adaline\n",
    "\n",
    "A multiclass Adaline is based on a layer of Adalines taking `n_in` input signals and distributing them to `n_out` output signals. The main difference to our preceding implementation of the Adaline is that now the weights are collected in a matrix `W_` instead of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClass: \n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        '''\n",
    "        initialize class for `num` input signals\n",
    "        '''\n",
    "\n",
    "        self.n_in_ = n_in\n",
    "        self.n_out_ = n_out\n",
    "\n",
    "        self.W_ = np.zeros( (n_out, n_in) )\n",
    "        self.b_ = np.zeros( (n_out, 1) )\n",
    "\n",
    "        return\n",
    "    \n",
    "    def activation_input(self, X):\n",
    "        '''\n",
    "        calculate the activation input of the neuron\n",
    "        '''\n",
    "        return np.dot(X, self.W_.T) + self.b_.T\n",
    "    \n",
    "    def softmax(self,X):\n",
    "        '''\n",
    "        calculate activation function (softmax)\n",
    "        '''\n",
    "        exp = np.exp(self.activation_input(X))\n",
    "        \n",
    "        return exp / exp.sum(axis=1).reshape((X.shape[0],1))\n",
    "    \n",
    "    def crossentropy(self,X,Y):\n",
    "        '''\n",
    "        computes cross entropy for loss funcion\n",
    "        '''      \n",
    "        return -(1 / X.shape[0]) * (Y * np.log(self.softmax(X)) + (1 - Y) * np.log(1 - self.softmax(X))).sum()\n",
    "    \n",
    "    def classify(self, X):\n",
    "        '''\n",
    "        classify the data by sending the softmax output through an argmax\n",
    "        '''\n",
    "        return self.softmax(X).argmax(axis=1)\n",
    "\n",
    "    def efficiency(self, X_test, Y_test): \n",
    "        '''\n",
    "        compute the efficiency = 1 - number of misclassifications / number of data points\n",
    "        '''\n",
    "\n",
    "        err = 0\n",
    "        \n",
    "        # classify the test data\n",
    "        Z = self.classify(X_test)\n",
    "        for z, y in zip(Z, Y_test):\n",
    "            err += int(z != y)\n",
    "\n",
    "        return 1 - float(err) / len(X_test)\n",
    "\n",
    "    def learn(self, X_train, Y_train, eta=0.01, epochs=1000):\n",
    "        '''\n",
    "        fit training data according to eta and n_iter\n",
    "        and log the errors in errors_\n",
    "        '''\n",
    "\n",
    "        def label2vec(y):\n",
    "            vecs = []\n",
    "            for l in y:\n",
    "                vec = np.zeros(self.n_out_)\n",
    "                vec[l] = 1\n",
    "                vecs.append(vec)\n",
    "                \n",
    "            return np.array(vecs)\n",
    "\n",
    "        # we initialize two lists, each for the misclassifications and the cost function\n",
    "        self.train_errors_ = []\n",
    "        self.train_loss_ = []\n",
    "\n",
    "        # for all the epochs\n",
    "        for _ in range(epochs):\n",
    "            # classify the traning features\n",
    "            Z = self.classify(X_train)\n",
    "            # count the misqualifications for the logging\n",
    "            err = 0\n",
    "            for z, y in zip(Z, Y_train):\n",
    "                err += int(z != y)\n",
    "            # ans save them in the list for later use\n",
    "            self.train_errors_.append(err)\n",
    "            \n",
    "            #the commented part is the old (quadratic loss, argmax) update rule\n",
    "            ''' \n",
    "            # compute the activation input of the entire traning features\n",
    "            output = self.activation_input(X_train)\n",
    "            # and then the deviation from the labels\n",
    "            delta = label2vec(Y_train) - output\n",
    "            # the following is an implmentation of the adaline update rule\n",
    "\n",
    "            self.W_ += eta * np.dot(X_train.T, delta).T / len(X_train[:,0])\n",
    "            self.b_ += eta * delta.sum(axis=0).reshape(self.n_out_,1) / len(X_train[:,0])\n",
    "            '''            \n",
    "            \n",
    "            # here comes the softmax-argmax update using the cross-entropy loss function\n",
    "            \n",
    "            # compute row of derivatives for W (column-independent)\n",
    "            deriv_W_ = np.empty_like(self.W_)\n",
    "            #Compute alpha_j - y_j\n",
    "            diff_ = label2vec(Y_train) - self.softmax(X_train)\n",
    "            \n",
    "            #update components of W_ \n",
    "            for j in range(deriv_W_.shape[0]):\n",
    "                for k in range(deriv_W_.shape[1]):\n",
    "                    deriv_W_[j,k] =  (diff_[:,j] * X_train[:,k]).sum(axis=0)\n",
    "            \n",
    "            self.W_ += eta * (1 / X_train.shape[0]) * np.array(deriv_W_)\n",
    "            \n",
    "            self.b_ += eta * (1 / X_train.shape[0]) * (diff_).sum(axis=0).reshape(self.b_.shape)\n",
    "            \n",
    "            \n",
    "            # and finally, we record the loss function\n",
    "            #for quadratic loss:\n",
    "            #loss = (delta ** 2).sum() / 2.0\n",
    "            \n",
    "            #cross entropy loss function\n",
    "            loss = self.crossentropy(X_train,label2vec(Y_train))\n",
    "            # and save it for later use\n",
    "            self.train_loss_.append(loss)\n",
    "\n",
    "        return\n",
    "\n",
    "    # the following two functions cannot be applied to the digit dataset\n",
    "    def plot_data(self, X, Y, n_classes, m):\n",
    "\n",
    "        # color map\n",
    "        colors = matplotlib.cm.rainbow(np.linspace(0, 1, n_classes))\n",
    "\n",
    "        # plot data with colors according to class labels\n",
    "        for l, c in zip(range(n_classes), colors):\n",
    "            xs = []\n",
    "            for xi, yi in zip(X, Y):\n",
    "                if yi == l:\n",
    "                    xs.append(xi)\n",
    "            xs = np.array(xs)\n",
    "            plt.scatter(xs[:,0], xs[:,1], color=c, marker=m, edgecolor='black')\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def plot_decision_regions(self, X, Y, X_train, Y_train, n_classes, resolution):\n",
    "\n",
    "        # set up a 2d mesh of data points with resolution `resolution`\n",
    "        x1_min, x1_max = X[:, 0].min() - 2, X[:, 0].max() + 2\n",
    "        x2_min, x2_max = X[:, 1].min() - 2, X[:, 1].max() + 2\n",
    "\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                               np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "        # start new plot\n",
    "        fig = plt.figure()\n",
    "        axs = plt.gca()\n",
    "\n",
    "        # make fictitious feature data out of the above 2d mesh\n",
    "        x_mesh = np.array( [xx1.ravel(), xx2.ravel()] ).T\n",
    "        # let the perceptron classify these features\n",
    "        Z = self.classify(x_mesh)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "\n",
    "        # plot the mesh as contour plot\n",
    "        axs.contourf(xx1, xx2, Z, alpha=0.1, cmap='prism')\n",
    "        axs.set_xlim(xx1.min(), xx1.max())\n",
    "        axs.set_ylim(xx2.min(), xx2.max())\n",
    "\n",
    "        # plot training data with 'x's\n",
    "        self.plot_data(X_train, Y_train, n_classes, 'x')\n",
    "        # plot unknown data with 'o's\n",
    "        self.plot_data(X, Y, n_classes, 'o')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read local data\n",
    "df = pd.read_csv(digit_name, header=None)\n",
    "dl = pd.read_csv(label_name,header=None)\n",
    "\n",
    "X_all, Y_all = prep_digit_data(df,dl)\n",
    "\n",
    "#rescale brightness values to prevent overflow in exp\n",
    "X_all = X_all/255\n",
    "\n",
    "# 28*28 pixels as input, 10 digit classes as output\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "\n",
    "#initialize instance of neuron layer\n",
    "mc = MultiClass(n_features,n_classes)\n",
    "\n",
    "# training data - choose size of training/test set\n",
    "train_samples = int( len(X_all) / 10 )\n",
    "X_train, Y_train = X_all[:train_samples], Y_all[:train_samples]\n",
    "\n",
    "# data for testing the efficiency\n",
    "X, Y = X_all[train_samples:], Y_all[train_samples:]\n",
    "\n",
    "# initialize W and b (zero by default)\n",
    "mc.W_ = 0.0 * np.ones_like(mc.W_)\n",
    "mc.b_ = 0.0 * np.ones_like(mc.b_)\n",
    "\n",
    "mc.learn(X_train, Y_train, eta=0.25, epochs=2000)\n",
    "\n",
    "#plot errors and loss function\n",
    "plt.plot(mc.train_errors_,'b')\n",
    "plt.show()\n",
    "plt.plot(mc.train_loss_,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current efficiency on test set to beat: 0.906\n",
    "\n",
    "Parameters: 10% of dataset as training data, 2000 epochs, eta = 0.25, brightness rescaling 1/255, W and b initialized as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficiencies of training\n",
    "eff_train = mc.efficiency(X_train, Y_train)\n",
    "eff_test = mc.efficiency(X, Y)\n",
    "eff_all = mc.efficiency(X_all, Y_all)\n",
    "\n",
    "print('Efficiency (train) =', eff_train)\n",
    "print('Efficiency (test)  =', eff_test)\n",
    "print('Efficiency (all)   =', eff_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with random digit\n",
    "\n",
    "nmbr = np.random.randint(0,len(X))\n",
    "sample = X[nmbr].reshape(28,28)\n",
    "\n",
    "plt.matshow(sample, cmap='gray')\n",
    "print(\"System recognized {}\\n Digit was {}\".format(mc.classify(X)[nmbr],Y[nmbr][0]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
